---
title: "Estimating a High Quantile for Insurance Claims"
author: "Justin Patterson"
header-includes:
   - \usepackage{amssymb}
output:
  html_document:
    df_print: paged
    toc: TRUE
    toc_float:
      collapsed: false
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding,
  output_file = "index.html") })
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(ggplot2)
library(lubridate)
library(dplyr)
library(kableExtra)
library(scales)
library(boot)
library(parallel)
library(purrr)


source("CDF-grapher.R")
source("clopper-pearson-CI.R")
source("log-likelihood.R")
source("mixture-distribution.R")
source("generalized-pareto.R")

set.seed(123)

# Constants
p = 0.8 # Our goal is a confidence interval for the p-quantile.
```

# Data Exploration
Our data consists of day-over-day insurance claims made after fire damage. We will assume that our data comprises an independent and identically distributed random sample.


```{r echo=FALSE}
# Import data.
prop_loss = read.csv("PropLoss.csv")
prop_loss$Date = as.Date.character(prop_loss$Date, format = "%m/%d/%Y")
total_loss = prop_loss$Total.Loss
total_loss = sort(total_loss)

# Group by year and month. We might want to know the p-quantile by year and month as well.
prop_loss = prop_loss %>%
  mutate(month = month(Date)) %>%
  mutate(year = year(Date))

prop_loss_grouped = prop_loss %>% 
  group_by(year, month) %>%
  summarize(total_loss = sum(Total.Loss), .groups = "keep") 
```

Our data consists of the total loss claimed by day from `r min(prop_loss$Date)` to `r max(prop_loss$Date)` (`r round(interval(min(prop_loss$Date), max(prop_loss$Date)) / years(1), 2)` years).

Let's look at some summary statistics of the total daily loss.

```{r}
# We use the population quantile estimator recommended by Hyndman and Fan (1996).
summary(total_loss, quantile.type = 8) 
```

Notice that the range of the total daily loss is in the millions. 
Let's naively try making a histogram.

```{r}
hist(total_loss, main = "Total Daily Loss", xlab = "Total Daily Loss")
```

The total daily loss looks extremely right-skewed. How often did we have a total daily loss of $0?

```{r}
num_days = length(total_loss)
num_zero_days = length(total_loss[total_loss == 0])
proportion_zero_days =  num_zero_days/num_days
```

There were `r num_zero_days` days with zero claims. This is `r round(proportion_zero_days * 100, 2)`% of the days.

Let's look at the sum of total daily loss per month.

```{r echo = FALSE}
ggplot(data = prop_loss_grouped,
       aes(x = total_loss)) +
  geom_histogram(aes(y = ..density..), bins = 25, color = "black") +
  geom_density(aes(y = ..density..)) +
  labs(title = "Distribution of the Total Daily Loss per Month", 
       x = "Total Daily Loss per Month (USD)",
       y = "Density") +
  theme(axis.text.y = element_blank(), 
        axis.ticks.y = element_blank())
```

Do some months have more days with non-zero claims than other months?

```{r month-comparison, echo = FALSE}
prop_loss_grouped_2 = prop_loss %>%
  group_by(month) %>%
  summarize(total_days = n(), 
            num_non_zero_days = length(Total.Loss[Total.Loss != 0]),
            num_zero_days = length(Total.Loss[Total.Loss == 0]), 
            .groups = "drop") %>%
  mutate(prop_non_zero_days = num_non_zero_days/total_days)

prop_loss_grouped_2$month = replace(x = prop_loss_grouped_2$month, list = prop_loss_grouped_2$month, values = month.abb)

# Order the months
prop_loss_grouped_2$month = ordered(prop_loss_grouped_2$month, levels = prop_loss_grouped_2$month[order(prop_loss_grouped_2$prop_non_zero_days)])

ggplot(data = prop_loss_grouped_2,
       aes(x = month,
           y = prop_non_zero_days)) +
  geom_col(fill = "maroon") +
  geom_text(aes(label = round(prop_non_zero_days, 2)), nudge_y = -0.05, color = "white") +
  labs(title = "Proportion of Non-Zero Claim Days by Month", y = "Proportion") +
    theme(axis.title.x = element_blank())
```

It appears that the proportion of non-zero claim days differs from month to month.

Let's test the null hypothesis that the proportion of non-zero claim days is the same for each month in the timespan that the data was collected vs. the alternative that the proportions are not all the same.

```{r}
prop.test(x = prop_loss_grouped_2$num_non_zero_days, n = prop_loss_grouped_2$total_days)
```

It appears likely at the 1% significance level that the proportion of non-zero claim days is not the same per month. In other words, there is an association between month and fire risk.

For the purposes of the present analysis, we will only try to estimate the p-quantile of total daily loss without reference to the month. In future studies, a hierarchical model could perhaps be built that looks at the number of claims that occur per month and uses that information to estimate a p-quantile on a month-by-month basis. Future studies could perhaps make use of the multinomial distribution and quantile regression.

## Skewness

Let's compute \(G_{1}\), the Adjusted Fisherâ€“Pearson Standardized Moment Coefficient^[[1]](https://en.wikipedia.org/wiki/Skewness#Sample_skewness)^ to get an idea of the population skewness.

\[
G_{1} = \frac{\sqrt{n\left(n-1\right)}}{n-2} \cdot \frac{\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{3}}{\left[\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right]^{3/2}}
\]

```{r echo = FALSE}
mean_claim = mean(total_loss)
sqrt(num_days*(num_days - 1))/(num_days-2)*(1/num_days*sum((total_loss - mean_claim)^3))/(1/num_days*sum((total_loss - mean_claim)^2))^(3/2)
```

For comparison, a symmetric distribution would have a skewness of 0. An exponential distribution would have a skewness of 2.

## Excess Kurtosis
Excess kurtosis measures the heaviness of the tails of a distribution in relation to the normal distribution. Distributions with fat tails will have a positive excess kurtosis.^[[2]](https://en.wikipedia.org/wiki/Kurtosis#Excess_kurtosis)^ We expect our distribution to have positive excess kurtosis because both high-valued properties and accidents are rare. We will estimate the population excess kurtosis by using the statistic

\[
G_{2} = 
\frac{\left(n+1\right)n\left(n-1\right)}
    {\left(n-2\right)\left(n-3\right)}
\cdot
\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{4}}
    {\left[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right]^{2}}
-3
\cdot
\frac{\left( n-1 \right)^{2}}
    {\left( n-2 \right)\left( n-3 \right)}
\]

```{r echo = FALSE}
((num_days + 1) * (num_days) * (num_days - 1)) /
((num_days - 2) * (num_days - 3)) *
sum((total_loss - mean_claim)^4) /
(sum((total_loss - mean_claim)^2))^2 - 3 *
((num_days - 1)^2) /
((num_days - 2) * (num_days - 3))
```

This high positive value for the excess kurtosis indicates that our distribution has heavy tails. This is often the case for non-life insurance claims.

## Is Total Daily Loss Continuous or Discrete?

Is the total daily loss rounded to the nearest dollar?

```{r dollar-test}
is.integer(total_loss)
```

Is the total daily loss rounded?

```{r 2-digit-rounding, echo = FALSE}
# create a function to extract the last n digits from a numeric vector, x.
bben_tail <- function(x, n){
    sapply(
      lapply(
        strsplit(
          as.character(x), split = ""
        ), 
        FUN = tail, 
        n = n
      ),
    paste,
    collapse = ""
    )
}

# Extract the last 2 digits (if possible) from each total loss.
last_2_digits <- sapply(total_loss, bben_tail, n = 2) 

last_2_digits_tally = c(sum(last_2_digits == "0"),
                                        sum(last_2_digits == "00"),
                                        sum(last_2_digits != "0" & last_2_digits != "00")
)

last_2_digits_df = data.frame(last_2 = c("0", "00", "other"),
                              count = last_2_digits_tally,
                              prop = round(last_2_digits_tally/num_days, 4)
)

last_2_digits_df %>% kable(col.names = c("Last Digit(s) of Total Loss", "Number of Days", "Proportion")) %>% kable_styling(full_width = FALSE)
```

We can see that it is very common for the total daily loss to be rounded to the nearest $100. What happens when we admit the last 3 digits into consideration?

```{r 3-digit-rounding, echo = FALSE}
# Extract the last 3 digits from each total daily loss
last_3_digits <- sapply(total_loss, bben_tail, n = 3) 

last_3_digits_tally = c(sum(last_3_digits == "0"),
                                        sum(last_3_digits == "00"),
                                        sum(last_3_digits == "000"),
                                        sum(last_3_digits == "500"),
                                        sum(last_3_digits != "0" & last_3_digits != "00" & last_3_digits != "000" & last_3_digits != "500")
)

last_3_digits_df = data.frame(last_3 = c("0", "00", "000", "500", "other"),
                              count = last_3_digits_tally,
                              proportion = round(last_3_digits_tally/num_days, 4)
)

last_3_digits_df %>% kable(col.names = c("Last Digit(s) of Total Loss", "Number of Days", "Proportion")) %>% kable_styling(full_width = FALSE)
```

Now we see that many total losses are in units of 500 or 1000 USD. **For simplicity, we will ignore the possibly discrete nature of the total daily loss.**

## Empirical CDF

```{r}
total_loss_ecdf = ecdf(total_loss)
plot(total_loss_ecdf, main = "Empirical Cumulative Distribution Function\n of Total Daily Loss")
```

We can zoom in around the `r p`-quantile.

```{r}
# This finds the p-quantile of our sample data.
p_quantile = quantile(total_loss_ecdf, probs = p, type = 8) # We use the population quantile estimator recommended by Hyndman and Fan (1996).

plot(total_loss_ecdf, main = "Empirical Cumulative Distribution Function\n of Total Loss", xlim = c(2e4,6e4), ylim = c(0.7, 0.9))
abline(h = p, v = p_quantile, col = "red")

plot(total_loss_ecdf, main = "Empirical Cumulative Distribution Function\n of Total Daily Loss", xlim = c(3.1e4,3.2e4), ylim = c(0.798, 0.802))
abline(h = p, v = p_quantile, col = "red")
```

**Our point estimate of the `r p`-quantile is `r dollar(p_quantile)`.**

# Quantile Estimation Using the Empirical CDF

The Dvoretzkyâ€“Kieferâ€“Wolfowitz Inequality^[[3]](https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality)^ allows for the construction of simultaneous confidence bands around the Emperical CDF. The inequality is non-parametric so it is not necessary to first fit a distribution to the total daily loss. We will choose \(\alpha = 0.05\). The inequality states:

\[
P\left(\sup_{x \in \mathbb{R}} \left| F_{n} \left( x \right) - F \left( x \right)\right| > \epsilon \right) \le 2e^{-2n\epsilon ^{2}} = \alpha
\]

or, with probability \(1 - \alpha\), the interval containing the true CDF is 

\[
F_{n} \left(x \right) - \epsilon \leq F \left(x \right) 
\leq F_{n} \left(x \right) + \epsilon \quad \text{where} \ \epsilon = \sqrt{\frac{\ln \frac{2}{\alpha}}{2n}}
\]

```{r}
alpha = 0.05
epsilon = sqrt(log(2/alpha)/(2*num_days))
```

For our problem, \(\epsilon =\) `r round(epsilon, 4)`. 

```{r echo=FALSE, warning=FALSE}
# Make a graph.
check_CDF(data = total_loss, alpha_for_DKW = 0.05) +
  xlim(2e4,6e4) + 
  ylim(0.7,0.925) +
  geom_segment(aes(x = -Inf, y = p, xend = 31500, yend = p), linetype = 2, col = "blue") +
  # geom_segment(aes(x = 36000, y = p, xend = 36000, yend = -Inf), linetype = 2, col = "blue") +
  # geom_segment(aes(x = 28000, y = p, xend = 28000, yend = -Inf), linetype = 2, col = "blue") +
  geom_segment(aes(x = 31500, y = 0.78, xend = 31500, yend = 0.822), linetype = 2, col = "green") +
  geom_segment(aes(x = 31500, y = 0.822, xend = 36500, yend = 0.822), linetype = 2, col = "green") +
  geom_segment(aes(x = 28500, y = 0.78, xend = 31500, yend = 0.78), linetype = 2, col = "green") +
  geom_segment(aes(x = 28500, y = 0.78, xend = 28500, yend = -Inf), linetype = 2, col = "green") +
  geom_segment(aes(x = 36500, y = 0.822, xend = 36500, yend = -Inf), linetype = 2, col = "green")

  # geom_vline(xintercept = 28500, col = "green") +
  # geom_vline(xintercept = 36500, col = "green")
```

One weakness of the Dvoretzkyâ€“Kieferâ€“Wolfowitz bands is that their coverage probability is lower near the median of the distribution while higher near the tails. However, these bands very easily capture where the true CDF may lie.

### Obtaining a Pointwise Confidence Interval Using the Empirical CDF
From the convergence of the Empirical CDF, [[4]](https://www.win.tue.nl/~rmcastro/AppStat2013/files/lecture1.pdf), it has been shown that

\[
n \hat{F}_{n} \left( x \right) \sim \text{Binom}\left( n, F \left( x \right) \right)
\]

We can use our preferred method of constructing a CI for a binomial success probability to obtain a point-wise CI for \(F(x)\). Note that our point-estimate for the true CDF in these intervals will be the Empirical CDF. We will try to construct an exact Clopper-Pearson CI because it is conservative.

Note that the Hyndman and Fan (1996) Type 8 Estimate of the p-quantile uses two order statistics. Therefore, our point-estimate of the p-quantile corresponds to not one, but to two order statistics. There is thus no obvious way to map our estimate back to a single order statistic. We choose the larger of the two order statistics to give an upward bias to our confidence interval. We feel that it is a greater cause for concern if the actual p-quantile is above the upper bound than if it is below the lower bound.

```{r}
k = floor(num_days*p + (p + 1)/3) + 1 # k is the index for the larger order stat

# Get the upper and lower bounds on the true CDF corresponding to k.
CDF_bounds = get_clopper_pearson(x = k, n = num_days, alpha = 0.05)

# Transform the bounds obtained above into quantiles of daily total loss using the Hyndman and Fan (1996) type 8 quantile estimator.
CI = data.frame(Method = "Binomial CI on ECDF",
                Nominal_Coverage_Probability = 1 - alpha,
                LB = round(quantile(x = total_loss, probs = CDF_bounds[1], type = 8, names = FALSE), 2),
                UB = round(quantile(x = total_loss, probs = CDF_bounds[2], type = 8, names = FALSE), 2),
                row.names = NULL
)
```

**We are 95% confident that the `r p`-quantile lies between `r dollar(CI$LB)` and `r dollar(CI$UB)`.** 

# Bootstrapping the p-quantile--a Non-Parametric Approach

```{r Bootstrapping-Non-Parametric}
# This function takes a vector of numeric sample data and finds the estimate of the p-quantile of the corresponding population according to the method of Ivan Frohne and Rob J Hyndman.
find_qp = function(data, i, p) {
  quantile(data[i], p, type = 8) # Approximately median-unbiased
}

# Reduce the number of replications to speed up computation time.
R = 1e4
cpus_on_machine = detectCores()
bootstrap_results_1 = boot(data = total_loss, statistic = find_qp, p = p, R = R, sim = "ordinary", parallel = "snow", ncpus = cpus_on_machine)

alpha = 0.05
CI[2, "Method"] = "Percentile Bootstrap"
CI[2, "Nominal_Coverage_Probability"] = 1 - alpha
CI[2, "LB"] = round(quantile(bootstrap_results_1$t[,1], p = alpha/2, type = 8, names = FALSE), 0)
CI[2, "UB"] = round(quantile(bootstrap_results_1$t[,1], p = 1 - alpha/2, type = 8, names = FALSE), 0)

hist(bootstrap_results_1$t[,1], xlim = c(2.8e4, 3.6e4), nclass = 30, xlab = paste0(p, "-Quantile of Resampled Total Loss"), main = paste0("Percentile Bootstrap Estimate of ", p, "-Quantile of Total Loss\nR = ", R))
abline(v = CI[2, "LB"], col = "red")
abline(v = CI[2, "UB"], col = "red")
```

This compares favorably with the CI obtained using the Empirical CDF.

```{r}
# We can also get a Basic Bootstrap CI for comparison.
CI[3, "Method"] = "Basic Bootstrap"
CI[3, "Nominal_Coverage_Probability"] = 1 - alpha
CI[3, "LB"] = 2 * p_quantile - round(quantile(bootstrap_results_1$t[,1], p = 1 - alpha/2, type = 8, names = FALSE), 0)
CI[3, "UB"] = 2 * p_quantile - round(quantile(bootstrap_results_1$t[,1], p = alpha/2, type = 8, names = FALSE), 0)
```

# Summary

```{r summary,echo=FALSE}
CI$Length = with(CI, UB - LB)
CI$Shape = with(CI, round((UB - p_quantile)/(p_quantile - LB), 2))

CI %>% kable(col.names = c("Method", "Nominal Coverage Probability", "LB", "UB", "Length", "Shape")) %>% kable_styling(full_width = FALSE)
```


# References
1. [Adjusted Fisherâ€“Pearson Standardized Moment Coefficient](https://en.wikipedia.org/wiki/Skewness#Sample_skewness)

2. [Excess Kurtosis](https://en.wikipedia.org/wiki/Kurtosis#Excess_kurtosis)

3. [Dvoretzkyâ€“Kieferâ€“Wolfowitz Inequality](https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality)

4. [Using the Empirical CDF
](https://www.win.tue.nl/~rmcastro/AppStat2013/files/lecture1.pdf)

5. [Bootstrapping](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf)

6. [Bootstrapping](http://users.stat.umn.edu/~helwig/notes/bootci-Notes.pdf)
